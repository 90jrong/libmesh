Index: contrib/patches/serialized_io_refactor.patch
===================================================================
--- contrib/patches/serialized_io_refactor.patch	(revision 6296)
+++ contrib/patches/serialized_io_refactor.patch	(working copy)
@@ -1,766 +0,0 @@
-Index: include/parallel/parallel_implementation.h
-===================================================================
---- include/parallel/parallel_implementation.h	(revision 6294)
-+++ include/parallel/parallel_implementation.h	(working copy)
-@@ -1047,13 +1047,13 @@
-     }
-   used_tag_values[tagvalue] = 1;
- 
--#ifndef NDEBUG
--  // Make sure everyone called get_unique_tag and make sure
--  // everyone got the same value
--  int maxval = tagvalue;
--  this->max(maxval);
--  libmesh_assert_equal_to (tagvalue, maxval);
--#endif
-+// #ifndef NDEBUG
-+//   // Make sure everyone called get_unique_tag and make sure
-+//   // everyone got the same value
-+//   int maxval = tagvalue;
-+//   this->max(maxval);
-+//   libmesh_assert_equal_to (tagvalue, maxval);
-+// #endif
- 
-   return MessageTag(tagvalue, this);
- }
-Index: include/systems/system.h
-===================================================================
---- include/systems/system.h	(revision 6294)
-+++ include/systems/system.h	(working copy)
-@@ -1110,6 +1110,13 @@
- 			      const bool write_additional_data = true) const;
- 
-   /**
-+   *
-+   */
-+  unsigned int write_serialized_vectors (Xdr &io,
-+					 const std::vector<std::string> &names,
-+					 const std::vector<NumericVector<Number>*> &vectors) const;
-+     
-+  /**
-    * Writes additional data, namely vectors, for this System.
-    * This method may safely be called on a distributed-memory mesh.
-    * This method will create an individual file for each processor in the simulation
-@@ -1516,8 +1523,8 @@
-    * Reads a vector for this System.
-    * This method may safely be called on a distributed-memory mesh.
-    */
--  void read_serialized_vector (Xdr& io,
--			       NumericVector<Number> &vec);
-+  unsigned int read_serialized_vector (Xdr& io,
-+				       NumericVector<Number> &vec);
- 
-   /**
-    * Writes an output vector to the stream \p io for a set of \p DofObjects.
-@@ -1532,6 +1539,17 @@
- 						     Xdr &io) const;
- 
-   /**
-+   * Writes an output vector to the stream \p io for a set of \p DofObjects.
-+   * This method uses blocked output and is safe to call on a distributed memory-mesh.
-+   */
-+  template <typename iterator_type>
-+  unsigned int write_serialized_blocked_dof_objects (const NumericVector<Number> &vec,
-+						     const unsigned int n_objects,
-+						     const iterator_type begin,
-+						     const iterator_type end,
-+						     Xdr &io) const;
-+
-+  /**
-    * Writes the SCALAR dofs associated with var to the stream \p io.
-    */
-   unsigned int write_SCALAR_dofs (const NumericVector<Number> &vec,
-@@ -1542,8 +1560,8 @@
-    * Writes a vector for this System.
-    * This method may safely be called on a distributed-memory mesh.
-    */
--  void write_serialized_vector (Xdr& io,
--				const NumericVector<Number> &vec) const;
-+  unsigned int write_serialized_vector (Xdr& io,
-+					const NumericVector<Number> &vec) const;
- 
-   /**
-    * Function that initializes the system.
-Index: src/systems/equation_systems_io.C
-===================================================================
---- src/systems/equation_systems_io.C	(revision 6294)
-+++ src/systems/equation_systems_io.C	(working copy)
-@@ -256,6 +256,9 @@
- 	iss >> ver_major >> dot >> ver_minor >> dot >> ver_patch;
- 	io.set_version(LIBMESH_VERSION(ver_major, ver_minor, ver_patch));
- 
-+	std::cout << "ver_major.ver_minor.ver_patch=" 
-+		  << ver_major << "." << ver_minor << "." << ver_patch
-+		  << std::endl;
- 
- 	read_parallel_files = (version.rfind(" parallel") < version.size());
- 
-Index: src/systems/system_io.C
-===================================================================
---- src/systems/system_io.C	(revision 6294)
-+++ src/systems/system_io.C	(working copy)
-@@ -475,6 +475,10 @@
-    * ASCII output.  Thus this one section of code will read XDR or ASCII
-    * files with no changes.
-    */
-+  PerfLog pl("IO Performance",false);
-+  pl.push("read_parallel_data");
-+  unsigned int total_read_size = 0;
-+  
-   libmesh_assert (io.reading());
-   libmesh_assert (io.is_open());
- 
-@@ -514,6 +518,8 @@
-   // for the ith system to disk
-   io.data(io_buffer);
- 
-+  total_read_size += io_buffer.size();
-+
-   const unsigned int sys_num = this->number();
-   const unsigned int n_vars  = this->_written_var_indices.size();
-   libmesh_assert_less_equal (n_vars, this->n_vars());
-@@ -590,6 +596,8 @@
- 	  // for the ith system to disk
- 	  io.data(io_buffer);
- 
-+	  total_read_size += io_buffer.size();
-+
- 	  // Loop over each non-SCALAR variable and each node, and read out the value.
-           for (unsigned int data_var=0; data_var<n_vars; data_var++)
-             {
-@@ -644,6 +652,16 @@
-           pos->second->close();
- 	}
-     }
-+
-+  const Real 
-+    dt   = pl.get_elapsed_time(),
-+    rate = total_read_size*sizeof(Number)/dt; 
-+
-+  std::cerr << "Read " << total_read_size << " \"Number\" values\n"
-+	    << " Elapsed time = " << dt << '\n'
-+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
-+
-+  pl.pop("read_parallel_data");
- }
- 
- 
-@@ -665,10 +683,15 @@
-   parallel_only();
-   std::string comment;
- 
-+  PerfLog pl("IO Performance",false);
-+  pl.push("read_serialized_data");
-+  unsigned int total_read_size = 0;
-+
-   // 10.)
-   // Read the global solution vector
-   {
--    this->read_serialized_vector(io, *this->solution);
-+    total_read_size +=
-+      this->read_serialized_vector(io, *this->solution);
- 
-     // get the comment
-     if (libMesh::processor_id() == 0)
-@@ -684,7 +707,8 @@
- 
-       for(; pos != this->_vectors.end(); ++pos)
-         {
--	  this->read_serialized_vector(io, *pos->second);
-+	  total_read_size +=
-+	    this->read_serialized_vector(io, *pos->second);
- 
- 	  // get the comment
- 	  if (libMesh::processor_id() == 0)
-@@ -692,6 +716,16 @@
- 
- 	}
-     }
-+
-+  const Real 
-+    dt   = pl.get_elapsed_time(),
-+    rate = total_read_size*sizeof(Number)/dt; 
-+
-+  std::cout << "Read " << total_read_size << " \"Number\" values\n"
-+	    << " Elapsed time = " << dt << '\n'
-+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
-+
-+  pl.pop("read_serialized_data");
- }
- 
- 
-@@ -894,7 +928,7 @@
- 
- 
- 
--void System::read_serialized_vector (Xdr& io, NumericVector<Number>& vec)
-+unsigned int System::read_serialized_vector (Xdr& io, NumericVector<Number>& vec)
- {
-   parallel_only();
- 
-@@ -966,6 +1000,8 @@
-   Parallel::sum (n_assigned_vals);
-   libmesh_assert_equal_to (n_assigned_vals, vector_length);
- #endif
-+
-+  return vector_length;
- }
- 
- 
-@@ -1408,6 +1444,10 @@
-    * ASCII output.  Thus this one section of code will read XDR or ASCII
-    * files with no changes.
-    */
-+  PerfLog pl("IO Performance",false);
-+  pl.push("write_parallel_data");
-+  unsigned int total_written_size = 0;
-+  
-   std::string comment;
- 
-   libmesh_assert (io.writing());
-@@ -1504,6 +1544,8 @@
- 
-   io.data (io_buffer, comment.c_str());
- 
-+  total_written_size += io_buffer.size();
-+
-   // Only write additional vectors if wanted
-   if (write_additional_data)
-     {
-@@ -1573,8 +1615,20 @@
- 	  }
- 
- 	  io.data (io_buffer, comment.c_str());
-+
-+	  total_written_size += io_buffer.size();
- 	}
-     }
-+
-+  const Real 
-+    dt   = pl.get_elapsed_time(),
-+    rate = total_written_size*sizeof(Number)/dt; 
-+
-+  std::cerr << "Write " << total_written_size << " \"Number\" values\n"
-+	    << " Elapsed time = " << dt << '\n'
-+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
-+  
-+  pl.pop("write_parallel_data");
- }
- 
- 
-@@ -1598,7 +1652,12 @@
-   parallel_only();
-   std::string comment;
- 
--  this->write_serialized_vector(io, *this->solution);
-+  PerfLog pl("IO Performance",false);
-+  pl.push("write_serialized_data");
-+  unsigned int total_written_size = 0;
-+  
-+  total_written_size +=
-+    this->write_serialized_vector(io, *this->solution);
- 
-   // set up the comment
-   if (libMesh::processor_id() == 0)
-@@ -1618,7 +1677,8 @@
- 
-       for(; pos != this->_vectors.end(); ++pos)
-         {
--	  this->write_serialized_vector(io, *pos->second);
-+	  total_written_size +=
-+	    this->write_serialized_vector(io, *pos->second);
- 
- 	  // set up the comment
- 	  if (libMesh::processor_id() == 0)
-@@ -1632,6 +1692,53 @@
- 	    }
- 	}
-     }
-+
-+  const Real 
-+    dt   = pl.get_elapsed_time(),
-+    rate = total_written_size*sizeof(Number)/dt; 
-+
-+  std::cout << "Write " << total_written_size << " \"Number\" values\n"
-+	    << " Elapsed time = " << dt << '\n'
-+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
-+  
-+  pl.pop("write_serialized_data");
-+
-+  
-+
-+  
-+  // test the new method
-+  {
-+    std::vector<std::string> names;
-+    std::vector<NumericVector<Number>*> vectors_to_write;
-+
-+    names.push_back("Solution Vector");
-+    vectors_to_write.push_back(this->solution.get());
-+    
-+    // Only write additional vectors if wanted
-+    if (write_additional_data)
-+      {
-+	std::map<std::string, NumericVector<Number>* >::const_iterator
-+	  pos = _vectors.begin();
-+	
-+	for(; pos != this->_vectors.end(); ++pos)
-+	  {
-+	    names.push_back("Additional Vector " + pos->first);
-+	    vectors_to_write.push_back(pos->second);
-+	  }
-+      }
-+
-+    total_written_size =
-+      this->write_serialized_vectors (io, names, vectors_to_write);
-+
-+    const Real 
-+      dt2   = pl.get_elapsed_time(),
-+      rate2 = total_written_size*sizeof(Number)/(dt2-dt);
-+    
-+    std::cout << "Write (new) " << total_written_size << " \"Number\" values\n"
-+	      << " Elapsed time = " << (dt2-dt) << '\n'
-+	      << " Rate = " << rate2/1.e6 << "(MB/sec)\n\n";
-+
-+  }
- }
- 
- 
-@@ -1804,6 +1911,305 @@
-   return written_length;
- }
- 
-+
-+
-+template <typename iterator_type>
-+unsigned int System::write_serialized_blocked_dof_objects (const NumericVector<Number> &vec,
-+							   const unsigned int n_objects,
-+							   const iterator_type begin,
-+							   const iterator_type end,
-+							   Xdr &io) const
-+{
-+  const unsigned int
-+    sys_num    = this->number(),
-+    num_vars   = this->n_vars(),    
-+    io_blksize = std::min(max_io_blksize, n_objects),
-+    num_blks   = std::ceil(static_cast<double>(n_objects)/static_cast<double>(io_blksize));
-+
-+  // std::cout << "io_blksize = "    << io_blksize
-+  // 	    << ", num_objects = " << n_objects
-+  // 	    << ", num_blks = "    << num_blks
-+  // 	    << std::endl;
-+  
-+  unsigned int written_length=0;                       // The numer of values written.  This will be returned
-+
-+  
-+
-+  std::vector<std::vector<unsigned int> > xfer_ids(num_blks);      // The global IDs and # of components for the local objects in all blocks
-+  std::vector<std::vector<Number> >       xfer_vals(num_blks);     // The raw values for the local objects in all blocks
-+  std::vector<Parallel::Request>
-+    id_requests(num_blks), val_requests(num_blks);                 // send request handle for each block
-+  
-+  // ------------------------------------------------------
-+  // First pass - count the number of objects in each block
-+  // traverse all the objects and figure out which block they
-+  // will utlimately live in.
-+  std::vector<unsigned int>
-+    xfer_ids_size  (num_blks,0),
-+    xfer_vals_size (num_blks,0);
-+  
-+  for (iterator_type it=begin; it!=end; ++it)
-+    {
-+      const unsigned int
-+	id    = (*it)->id(),
-+	block = id/io_blksize;
-+      
-+      libmesh_assert_less (block, num_blks);
-+      
-+      xfer_ids_size[block]  += (1 + num_vars); // for each object, we store its id, as well as the number of components for each variable
-+
-+      for (unsigned int var=0; var<num_vars; var++)
-+	xfer_vals_size[block] += (*it)->n_comp(sys_num, var); // for each variable, we will store the nonzero components
-+    }
-+
-+  //---------------------------------
-+  // Collect the values for all objects
-+  for (unsigned int blk=0; blk<num_blks; blk++)
-+    {
-+      //libMesh::out << "Writing object block " << blk << std::endl;
-+
-+      libmesh_assert_less (blk, num_blks);
-+
-+      // Each processor should build up its transfer buffers for its
-+      // local objects in [first_object,last_object).
-+      const unsigned int 
-+	first_object = blk*io_blksize,
-+	last_object  = std::min((blk+1)*io_blksize,n_objects);
-+
-+      // convenience
-+      std::vector<unsigned int> &ids  (xfer_ids[blk]);
-+      std::vector<Number>       &vals (xfer_vals[blk]);
-+
-+      // we now know the number of values we will store for each block,
-+      // so we can do efficient preallocation
-+      ids.clear();  /**/ ids.reserve  (xfer_ids_size[blk]);
-+      vals.clear(); /**/ vals.reserve (xfer_vals_size[blk]);
-+
-+      if (xfer_vals_size[blk] != 0) // this is possible - for example, may be called for a range of elements, with only nodal data
-+	for (iterator_type it=begin; it!=end; ++it)
-+	  if (((*it)->id() >= first_object) && // object in [first_object,last_object)
-+	      ((*it)->id() <   last_object))
-+	    {
-+	      ids.push_back((*it)->id());
-+	      
-+	      for (unsigned int var=0; var<num_vars; var++)
-+		{
-+		  const unsigned int n_comp = (*it)->n_comp(sys_num,var);
-+		  
-+		  ids.push_back (n_comp); // even if 0 - processor 0 has no way of knowing otherwise...
-+		  
-+		  for (unsigned int comp=0; comp<n_comp; comp++)
-+		    {
-+		      libmesh_assert_greater_equal ((*it)->dof_number(sys_num, var, comp), vec.first_local_index());
-+		      libmesh_assert_less ((*it)->dof_number(sys_num, var, comp), vec.last_local_index());
-+		      vals.push_back(vec((*it)->dof_number(sys_num, var, comp)));
-+		    }
-+		}
-+	    }
-+      
-+      Parallel::MessageTag id_tag  = Parallel::Communicator_World.get_unique_tag(100*num_blks + blk);
-+      Parallel::MessageTag val_tag = Parallel::Communicator_World.get_unique_tag(200*num_blks + blk);
-+      
-+      // send the data for this block
-+      Parallel::send (0, ids,  id_requests[blk],  id_tag);
-+      Parallel::send (0, vals, val_requests[blk], val_tag);
-+    }
-+
-+
-+  if (libMesh::processor_id() == 0)
-+    {
-+      std::vector<std::vector<unsigned int> > recv_ids  (libMesh::n_processors());
-+      std::vector<std::vector<Number> >       recv_vals (libMesh::n_processors());
-+      std::vector<std::vector<Number>::iterator>           // The next value on each processor for the current block
-+	val_iters(libMesh::n_processors());
-+      std::vector<unsigned int> idx_map;                   // map to traverse entry-wise rather than processor-wise
-+      std::vector<Number>       output_vals;              // The output buffer for the current block
-+
-+      const unsigned int idx_obj_size = (2+num_vars);
-+      
-+      for (unsigned int blk=0; blk<num_blks; blk++)
-+	{
-+	  // Each processor should build up its transfer buffers for its
-+	  // local objects in [first_object,last_object).
-+	  const unsigned int 
-+	    first_object = blk*io_blksize,
-+	    last_object  = std::min((blk+1)*io_blksize,n_objects);
-+
-+	  // Create a map to avoid searching.  This will allow us to
-+	  // traverse the received values in [first_object,last_object) order.
-+	  idx_map.resize(idx_obj_size*io_blksize); std::fill (idx_map.begin(), idx_map.end(), libMesh::invalid_uint);
-+	  
-+	  // receive this block of data from all processors.
-+	  for (unsigned int comm_step=0; comm_step<libMesh::n_processors(); comm_step++)
-+	    {
-+	      std::vector<unsigned int> &ids (recv_ids [comm_step]);
-+	      std::vector<Number>       &vals(recv_vals[comm_step]);
-+	      
-+	      Parallel::MessageTag id_tag  = Parallel::Communicator_World.get_unique_tag(100*num_blks + blk);
-+	      Parallel::MessageTag val_tag = Parallel::Communicator_World.get_unique_tag(200*num_blks + blk);
-+	      
-+	      // receive data for this block, imposing no particular order on processor
-+	      Parallel::receive (Parallel::any_source, ids,  id_tag);
-+	      Parallel::receive (Parallel::any_source, vals, val_tag);
-+	      
-+	      written_length += vals.size();
-+
-+	      if (vals.size()) // if we actually recevied any values
-+		{
-+		  val_iters[comm_step] = vals.begin();
-+		  for (unsigned int idx=0; idx<ids.size(); idx+=(1+num_vars))
-+		    {
-+		      const unsigned int local_idx = ids[idx+0]-first_object;
-+		      libmesh_assert_less (local_idx, std::min(io_blksize,n_objects));
-+
-+		      idx_map[idx_obj_size*local_idx+0] = comm_step;
-+		      idx_map[idx_obj_size*local_idx+1] = std::distance(vals.begin(), val_iters[comm_step]);
-+		      
-+		      for (unsigned int var=0; var<num_vars; var++)
-+			{
-+			  const unsigned int n_comp = ids[idx+var];
-+			  idx_map[idx_obj_size*local_idx+2+var] = n_comp;
-+			}
-+		    }
-+		}
-+	      else
-+		libmesh_here();
-+	      
-+	    }
-+	}
-+    }
-+
-+
-+  Parallel::wait(id_requests);
-+  Parallel::wait(val_requests);
-+  
-+  return written_length;
-+//   std::vector<unsigned int> xfer_ids;                  // The global IDs and # of components for the local objects in the current block
-+//   std::vector<Number>       xfer_vals;                 // The raw values for the local objects in the current block
-+//   std::vector<std::vector<unsigned int> >              // The global ID and # of components received from each processor
-+//     recv_ids (libMesh::n_processors());                //  for the current block
-+//   std::vector<std::vector<Number> >                    // The raw values received from each processor
-+//     recv_vals(libMesh::n_processors());                //  for the current block
-+//   std::vector<std::vector<Number>::iterator>           // The next value on each processor for the current block
-+//     val_iters;
-+//   val_iters.reserve(libMesh::n_processors());
-+//   std::vector<unsigned int> &idx_map     = xfer_ids;   // map to traverse entry-wise rather than processor-wise (renamed for notational convenience)
-+//   std::vector<Number>       &output_vals = xfer_vals;  // The output buffer for the current block (renamed for notational convenience)
-+
-+
-+//       //-----------------------------------------
-+//       // Send the transfer buffers to processor 0.
-+
-+//       // Get the size of the incoming buffers -- optionally
-+//       // we could over-size the recv buffers based on
-+//       // some maximum size to avoid these communications
-+//       std::vector<unsigned int> ids_size, vals_size;
-+//       const unsigned int my_ids_size  = xfer_ids.size();
-+//       const unsigned int my_vals_size = xfer_vals.size();
-+
-+//       Parallel::gather (0, my_ids_size,  ids_size);
-+//       Parallel::gather (0, my_vals_size, vals_size);
-+
-+//       // Note that we will actually send/receive to ourself if we are
-+//       // processor 0, so let's use nonblocking receives.
-+//       std::vector<Parallel::Request>
-+// 	id_request_handles(libMesh::n_processors()),
-+// 	val_request_handles(libMesh::n_processors());
-+
-+// #ifdef LIBMESH_HAVE_MPI
-+//       Parallel::MessageTag
-+//         id_tag    = Parallel::Communicator_World.get_unique_tag(2345),
-+//         val_tag = Parallel::Communicator_World.get_unique_tag(2346);
-+
-+//       // Post the receives -- do this on processor 0 only.
-+//       if (libMesh::processor_id() == 0)
-+// 	for (unsigned int pid=0; pid<libMesh::n_processors(); pid++)
-+// 	  {
-+// 	    recv_ids[pid].resize(ids_size[pid]);
-+// 	    recv_vals[pid].resize(vals_size[pid]);
-+
-+// 	    Parallel::nonblocking_receive (pid, recv_ids[pid],
-+// 					   id_request_handles[pid],
-+//                                            id_tag);
-+// 	    Parallel::nonblocking_receive (pid, recv_vals[pid],
-+// 					   val_request_handles[pid],
-+//                                            val_tag);
-+// 	  }
-+
-+//       // Send -- do this on all processors.
-+//       Parallel::send(0, xfer_ids,  id_tag);
-+//       Parallel::send(0, xfer_vals, val_tag);
-+// #else
-+//       // On one processor there's nothing to send
-+//       recv_ids[0] = xfer_ids;
-+//       recv_vals[0] = xfer_vals;
-+// #endif
-+
-+//       // -------------------------------------------------------
-+//       // Receive the messages and write the output on processor 0.
-+//       if (libMesh::processor_id() == 0)
-+// 	{
-+// 	  // Wait for all the receives to complete. We have no
-+// 	  // need for the statuses since we already know the
-+// 	  // buffer sizes.
-+// 	  Parallel::wait (id_request_handles);
-+// 	  Parallel::wait (val_request_handles);
-+
-+// 	  // Write the values in this block.
-+// 	  unsigned int tot_id_size=0, tot_val_size=0;
-+// 	  val_iters.clear();
-+// 	  for (unsigned int pid=0; pid<libMesh::n_processors(); pid++)
-+// 	    {
-+// 	      tot_id_size  += recv_ids[pid].size();
-+// 	      tot_val_size += recv_vals[pid].size();
-+// 	      val_iters.push_back(recv_vals[pid].begin());
-+// 	    }
-+
-+// 	  libmesh_assert_less_equal (tot_id_size, 2*std::min(io_blksize,n_objects));
-+
-+// 	  // Create a map to avoid searching.  This will allow us to
-+// 	  // traverse the received values in [first_object,last_object) order.
-+// 	  idx_map.resize(3*io_blksize); std::fill (idx_map.begin(), idx_map.end(), libMesh::invalid_uint);
-+// 	  for (unsigned int pid=0; pid<libMesh::n_processors(); pid++)
-+// 	    for (unsigned int idx=0; idx<recv_ids[pid].size(); idx+=2)
-+// 	      {
-+// 		const unsigned int local_idx = recv_ids[pid][idx+0]-first_object;
-+// 		libmesh_assert_less (local_idx, std::min(io_blksize,n_objects));
-+// 		const unsigned int n_comp    = recv_ids[pid][idx+1];
-+
-+// 		idx_map[3*local_idx+0] = pid;
-+// 		idx_map[3*local_idx+1] = n_comp;
-+// 		idx_map[3*local_idx+2] = std::distance(recv_vals[pid].begin(), val_iters[pid]);
-+// 		val_iters[pid] += n_comp;
-+// 	      }
-+
-+// 	  output_vals.clear(); output_vals.reserve (tot_val_size);
-+// 	  for (unsigned int idx=0; idx<idx_map.size(); idx+=3)
-+// 	    if (idx_map[idx] != libMesh::invalid_uint) // this could happen when a local object
-+// 	      {                                        // has no components for the current variable
-+// 		const unsigned int pid       = idx_map[idx+0];
-+// 		const unsigned int n_comp    = idx_map[idx+1];
-+// 		const unsigned int first_pos = idx_map[idx+2];
-+
-+// 		for (unsigned int comp=0; comp<n_comp; comp++)
-+// 		  {
-+// 		    libmesh_assert_less (first_pos + comp, recv_vals[pid].size());
-+// 		    output_vals.push_back(recv_vals[pid][first_pos + comp]);
-+// 		  }
-+// 	      }
-+// 	  libmesh_assert_equal_to (output_vals.size(), tot_val_size);
-+
-+// 	  // write the stream
-+// 	  io.data_stream (output_vals.empty() ? NULL : &output_vals[0], output_vals.size());
-+// 	  written_length += output_vals.size();
-+// 	} // end processor 0 conditional block
-+//     } // end object block loop
-+
-+//   return written_length;
-+}
-+
-+
-+
- unsigned int System::write_SCALAR_dofs (const NumericVector<Number> &vec,
-                                         const unsigned int var,
- 					Xdr &io) const
-@@ -1854,7 +2260,8 @@
- }
- 
- 
--void System::write_serialized_vector (Xdr& io, const NumericVector<Number>& vec) const
-+
-+unsigned int System::write_serialized_vector (Xdr& io, const NumericVector<Number>& vec) const
- {
-   parallel_only();
- 
-@@ -1900,8 +2307,58 @@
- 
-   if (libMesh::processor_id() == 0)
-     libmesh_assert_equal_to (written_length, vec_length);
-+
-+  return written_length;
- }
- 
-+
-+
-+unsigned int System::write_serialized_vectors (Xdr &io,
-+					       const std::vector<std::string> &names,
-+					       const std::vector<NumericVector<Number>*> &vectors) const
-+{
-+  parallel_only();
-+
-+  libmesh_assert (io.writing());
-+
-+  // Cache these - they are not free!
-+  const unsigned int
-+    n_nodes = this->get_mesh().n_nodes(),
-+    n_elem  = this->get_mesh().n_elem();  
-+
-+  unsigned int written_length = 0.;
-+  
-+  // Loop over each vetor and write it out, object-major
-+  for (std::vector<NumericVector<Number>*>::const_iterator vec_it=vectors.begin();
-+       vec_it!=vectors.end(); ++vec_it)
-+    {
-+      libmesh_assert_not_equal_to (*vec_it, NULL);
-+      const NumericVector<Number> &vec(**vec_it);
-+
-+      //---------------------------------
-+      // Collect the values for all nodes
-+      written_length +=
-+	this->write_serialized_blocked_dof_objects (vec,
-+						    n_nodes,
-+						    this->get_mesh().local_nodes_begin(),
-+						    this->get_mesh().local_nodes_end(),
-+						    io);
-+
-+      //------------------------------------
-+      // Collect the values for all elements
-+      written_length +=
-+	this->write_serialized_blocked_dof_objects (vec,
-+						    n_elem,
-+						    this->get_mesh().local_elements_begin(),
-+						    this->get_mesh().local_elements_end(),
-+						    io);
-+
-+      // and finally any scalars
-+    }
-+      
-+  return written_length;
-+}
-+
- } // namespace libMesh
- 
- 
-Index: examples/introduction/introduction_ex2/introduction_ex2.C
-===================================================================
---- examples/introduction/introduction_ex2/introduction_ex2.C	(revision 6294)
-+++ examples/introduction/introduction_ex2/introduction_ex2.C	(working copy)
-@@ -81,7 +81,7 @@
-   // 2D grid on the unit square.  By default a mesh of QUAD4
-   // elements will be created.  We instruct the mesh generator
-   // to build a mesh of 5x5 elements.
--  MeshTools::Generation::build_square (mesh, 5, 5);
-+  MeshTools::Generation::build_cube (mesh, 25, 25, 25);
- 
-   // Create an equation systems object. This object can
-   // contain multiple systems of different 
-@@ -124,8 +124,8 @@
-   // order.  Variables "c" and "T" will use first-order Lagrange approximation, 
-   // while variable "dv" will use a second-order discontinuous
-   // approximation space.
--  equation_systems.get_system("Complex System").add_variable("c", FIRST);
--  equation_systems.get_system("Complex System").add_variable("T", FIRST);
-+  equation_systems.get_system("Complex System").add_variable("c",  FIRST);
-+  equation_systems.get_system("Complex System").add_variable("T",  FIRST);
-   equation_systems.get_system("Complex System").add_variable("dv", SECOND, MONOMIAL);
-     
-   // Initialize the data structures for the equation system.
-@@ -150,22 +150,34 @@
-   if (argc > 1)
-     if (argv[1][0] != '-')
-       {
--        std::cout << "<<< Writing system to file " << argv[1]
-+        std::cout << "<<< Writing system to file " << "foo.xdr" //argv[1]
-                   << std::endl;
-         
-         // Write the system.
--        equation_systems.write (argv[1], libMeshEnums::WRITE);
-+	// std::cout << "ASCII\n";
-+        // equation_systems.write (argv[1],   libMeshEnums::WRITE);
-+	std::cout << "Binary\n";
-+        equation_systems.write ("foo.xdr", libMeshEnums::ENCODE,
-+				(//EquationSystems::WRITE_PARALLEL_FILES |
-+				 EquationSystems::WRITE_DATA |
-+				 EquationSystems::WRITE_ADDITIONAL_DATA));
-         
-         // Clear the equation systems data structure.
-         equation_systems.clear ();
- 
--        std::cout << ">>> Reading system from file " << argv[1]
-+        std::cout << ">>> Reading system from file " << "foo.xdr" //argv[1]
-                   << std::endl << std::endl;
-         
-         // Read the file we just wrote.  This better
-         // work!
--        equation_systems.read (argv[1], libMeshEnums::READ);
-+        //equation_systems.read (argv[1], libMeshEnums::READ);
-+	std::cout << "Binary\n";
-+        equation_systems.read ("foo.xdr", libMeshEnums::DECODE,			       
-+			       (EquationSystems::READ_HEADER |
-+				EquationSystems::READ_DATA |
-+				EquationSystems::READ_ADDITIONAL_DATA));
- 
-+
-         // Print the information again.
-         equation_systems.print_info();
-       }
Index: examples/introduction/introduction_ex2/introduction_ex2.C
===================================================================
--- examples/introduction/introduction_ex2/introduction_ex2.C	(revision 6296)
+++ examples/introduction/introduction_ex2/introduction_ex2.C	(working copy)
@@ -81,7 +81,7 @@
   // 2D grid on the unit square.  By default a mesh of QUAD4
   // elements will be created.  We instruct the mesh generator
   // to build a mesh of 5x5 elements.
-  MeshTools::Generation::build_square (mesh, 5, 5);
+  MeshTools::Generation::build_cube (mesh, 50, 50, 50);
 
   // Create an equation systems object. This object can
   // contain multiple systems of different 
@@ -124,8 +124,8 @@
   // order.  Variables "c" and "T" will use first-order Lagrange approximation, 
   // while variable "dv" will use a second-order discontinuous
   // approximation space.
-  equation_systems.get_system("Complex System").add_variable("c", FIRST);
-  equation_systems.get_system("Complex System").add_variable("T", FIRST);
+  equation_systems.get_system("Complex System").add_variable("c",  FIRST);
+  equation_systems.get_system("Complex System").add_variable("T",  FIRST);
   equation_systems.get_system("Complex System").add_variable("dv", SECOND, MONOMIAL);
     
   // Initialize the data structures for the equation system.
@@ -150,22 +150,34 @@
   if (argc > 1)
     if (argv[1][0] != '-')
       {
-        std::cout << "<<< Writing system to file " << argv[1]
+        std::cout << "<<< Writing system to file " << "foo.xdr" //argv[1]
                   << std::endl;
         
         // Write the system.
-        equation_systems.write (argv[1], libMeshEnums::WRITE);
+	// std::cout << "ASCII\n";
+        // equation_systems.write (argv[1],   libMeshEnums::WRITE);
+	std::cout << "Binary\n";
+        equation_systems.write ("foo.xdr", libMeshEnums::ENCODE,
+				(//EquationSystems::WRITE_PARALLEL_FILES |
+				 EquationSystems::WRITE_DATA |
+				 EquationSystems::WRITE_ADDITIONAL_DATA));
         
         // Clear the equation systems data structure.
         equation_systems.clear ();
 
-        std::cout << ">>> Reading system from file " << argv[1]
+        std::cout << ">>> Reading system from file " << "foo.xdr" //argv[1]
                   << std::endl << std::endl;
         
         // Read the file we just wrote.  This better
         // work!
-        equation_systems.read (argv[1], libMeshEnums::READ);
+        //equation_systems.read (argv[1], libMeshEnums::READ);
+	std::cout << "Binary\n";
+        equation_systems.read ("foo.xdr", libMeshEnums::DECODE,			       
+			       (EquationSystems::READ_HEADER |
+				EquationSystems::READ_DATA |
+				EquationSystems::READ_ADDITIONAL_DATA));
 
+
         // Print the information again.
         equation_systems.print_info();
       }
Index: include/parallel/parallel_implementation.h
===================================================================
--- include/parallel/parallel_implementation.h	(revision 6296)
+++ include/parallel/parallel_implementation.h	(working copy)
@@ -1030,13 +1030,13 @@
     }
   used_tag_values[tagvalue] = 1;
 
-#ifndef NDEBUG
-  // Make sure everyone called get_unique_tag and make sure
-  // everyone got the same value
-  int maxval = tagvalue;
-  this->max(maxval);
-  libmesh_assert_equal_to (tagvalue, maxval);
-#endif
+// #ifndef NDEBUG
+//   // Make sure everyone called get_unique_tag and make sure
+//   // everyone got the same value
+//   int maxval = tagvalue;
+//   this->max(maxval);
+//   libmesh_assert_equal_to (tagvalue, maxval);
+// #endif
 
   return MessageTag(tagvalue, this);
 }
Index: include/systems/system.h
===================================================================
--- include/systems/system.h	(revision 6296)
+++ include/systems/system.h	(working copy)
@@ -1104,6 +1104,13 @@
 			      const bool write_additional_data = true) const;
 
   /**
+   *
+   */
+  unsigned int write_serialized_vectors (Xdr &io,
+					 const std::vector<std::string> &names,
+					 const std::vector<NumericVector<Number>*> &vectors) const;
+     
+  /**
    * Writes additional data, namely vectors, for this System.
    * This method may safely be called on a distributed-memory mesh.
    * This method will create an individual file for each processor in the simulation
@@ -1499,6 +1506,20 @@
 						    NumericVector<Number> &vec) const;
 
   /**
+   * Reads an input vector from the stream \p io and assigns
+   * the values to a set of \p DofObjects.  This method uses
+   * blocked input and is safe to call on a distributed memory-mesh.
+   * Unless otherwise specified, all variables are read.
+   */
+  template <typename iterator_type>
+  unsigned int read_serialized_blocked_dof_objects (const unsigned int n_objects,
+						    const iterator_type begin,
+						    const iterator_type end,
+						    Xdr &io,
+						    NumericVector<Number> &vec,
+						    const unsigned int var_to_read=libMesh::invalid_uint) const;
+
+  /**
    * Reads the SCALAR dofs from the stream \p io and assigns the values
    * to the appropriate entries of \p vec.
    */
@@ -1510,8 +1531,8 @@
    * Reads a vector for this System.
    * This method may safely be called on a distributed-memory mesh.
    */
-  void read_serialized_vector (Xdr& io,
-			       NumericVector<Number> &vec);
+  unsigned int read_serialized_vector (Xdr& io,
+				       NumericVector<Number> &vec);
 
   /**
    * Writes an output vector to the stream \p io for a set of \p DofObjects.
@@ -1526,6 +1547,18 @@
 						     Xdr &io) const;
 
   /**
+   * Writes an output vector to the stream \p io for a set of \p DofObjects.
+   * This method uses blocked output and is safe to call on a distributed memory-mesh.
+   */
+  template <typename iterator_type>
+  unsigned int write_serialized_blocked_dof_objects (const NumericVector<Number> &vec,
+						     const unsigned int n_objects,
+						     const iterator_type begin,
+						     const iterator_type end,
+						     Xdr &io,
+						     const unsigned int var_to_write=libMesh::invalid_uint) const;
+
+  /**
    * Writes the SCALAR dofs associated with var to the stream \p io.
    */
   unsigned int write_SCALAR_dofs (const NumericVector<Number> &vec,
@@ -1536,8 +1569,8 @@
    * Writes a vector for this System.
    * This method may safely be called on a distributed-memory mesh.
    */
-  void write_serialized_vector (Xdr& io,
-				const NumericVector<Number> &vec) const;
+  unsigned int write_serialized_vector (Xdr& io,
+					const NumericVector<Number> &vec) const;
 
   /**
    * Function that initializes the system.
Index: src/systems/system_io.C
===================================================================
--- src/systems/system_io.C	(revision 6296)
+++ src/systems/system_io.C	(working copy)
@@ -22,6 +22,7 @@
 // C++ Includes
 #include <cstdio> // for std::sprintf
 #include <set>
+#include <numeric> // for std::partial_sum
 
 // Local Includes
 #include "libmesh/system.h"
@@ -475,6 +476,10 @@
    * ASCII output.  Thus this one section of code will read XDR or ASCII
    * files with no changes.
    */
+  PerfLog pl("IO Performance",false);
+  pl.push("read_parallel_data");
+  unsigned int total_read_size = 0;
+  
   libmesh_assert (io.reading());
   libmesh_assert (io.is_open());
 
@@ -514,6 +519,8 @@
   // for the ith system to disk
   io.data(io_buffer);
 
+  total_read_size += io_buffer.size();
+
   const unsigned int sys_num = this->number();
   const unsigned int n_vars  = this->_written_var_indices.size();
   libmesh_assert_less_equal (n_vars, this->n_vars());
@@ -590,6 +597,8 @@
 	  // for the ith system to disk
 	  io.data(io_buffer);
 
+	  total_read_size += io_buffer.size();
+
 	  // Loop over each non-SCALAR variable and each node, and read out the value.
           for (unsigned int data_var=0; data_var<n_vars; data_var++)
             {
@@ -644,6 +653,16 @@
           pos->second->close();
 	}
     }
+
+  const Real 
+    dt   = pl.get_elapsed_time(),
+    rate = total_read_size*sizeof(Number)/dt; 
+
+  std::cerr << "Read " << total_read_size << " \"Number\" values\n"
+	    << " Elapsed time = " << dt << '\n'
+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
+
+  pl.pop("read_parallel_data");
 }
 
 
@@ -665,10 +684,15 @@
   parallel_only();
   std::string comment;
 
+  PerfLog pl("IO Performance",false);
+  pl.push("read_serialized_data");
+  unsigned int total_read_size = 0;
+
   // 10.)
   // Read the global solution vector
   {
-    this->read_serialized_vector(io, *this->solution);
+    total_read_size +=
+      this->read_serialized_vector(io, *this->solution);
 
     // get the comment
     if (libMesh::processor_id() == 0)
@@ -684,7 +708,8 @@
 
       for(; pos != this->_vectors.end(); ++pos)
         {
-	  this->read_serialized_vector(io, *pos->second);
+	  total_read_size +=
+	    this->read_serialized_vector(io, *pos->second);
 
 	  // get the comment
 	  if (libMesh::processor_id() == 0)
@@ -692,6 +717,16 @@
 
 	}
     }
+
+  const Real 
+    dt   = pl.get_elapsed_time(),
+    rate = total_read_size*sizeof(Number)/dt; 
+
+  std::cout << "Read " << total_read_size << " \"Number\" values\n"
+	    << " Elapsed time = " << dt << '\n'
+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
+
+  pl.pop("read_serialized_data");
 }
 
 
@@ -846,6 +881,229 @@
   return n_assigned_vals;
 }
 
+
+
+template <typename iterator_type>
+unsigned int System::read_serialized_blocked_dof_objects (const unsigned int n_objects,
+							  const iterator_type begin,
+							  const iterator_type end,
+							  Xdr &io,
+							  NumericVector<Number> &vec,
+							  const unsigned int var_to_read) const
+{
+  const unsigned int
+    sys_num    = this->number(),
+    num_vars   = _written_var_indices.size(), // must be <= current numbe rof variables! 
+    io_blksize = std::min(max_io_blksize, n_objects),
+    num_blks   = std::ceil(static_cast<double>(n_objects)/static_cast<double>(io_blksize));
+
+  libmesh_assert_less_equal (num_vars, this->n_vars());
+
+  unsigned int n_read_values=0;
+  
+  std::vector<std::vector<unsigned int> > xfer_ids(num_blks);  // The global IDs and # of components for the local objects in all blocks
+  std::vector<std::vector<Number> >       recv_vals(num_blks); // The raw values for the local objects in all blocks
+  std::vector<Parallel::Request>       
+    id_requests(num_blks), val_requests(num_blks);
+
+  // ------------------------------------------------------
+  // First pass - count the number of objects in each block
+  // traverse all the objects and figure out which block they
+  // will utlimately live in.
+  std::vector<unsigned int>
+    xfer_ids_size  (num_blks,0),
+    recv_vals_size (num_blks,0);
+  
+  for (iterator_type it=begin; it!=end; ++it)
+    {
+      const unsigned int
+	id    = (*it)->id(),
+	block = id/io_blksize;
+      
+      libmesh_assert_less (block, num_blks);
+      
+      xfer_ids_size[block] += 2; // for each object, we send its id, as well as the total number of components for all variables
+
+      for (std::vector<unsigned int>::const_iterator var_it=_written_var_indices.begin();
+	   var_it!=_written_var_indices.end(); ++var_it)
+	recv_vals_size[block] += (*it)->n_comp(sys_num, *var_it); // for each variable, we will receive the nonzero components
+    }
+
+  //------------------------------------------
+  // Collect the ids & number of values needed
+  // for all local objects, binning them into 
+  // 'blocks' that will be sent to processor 0
+  for (unsigned int blk=0; blk<num_blks; blk++)
+    {
+      // Each processor should build up its transfer buffers for its
+      // local objects in [first_object,last_object).
+      const unsigned int 
+	first_object = blk*io_blksize,
+	last_object  = std::min((blk+1)*io_blksize,n_objects);
+
+      // convenience
+      std::vector<unsigned int> &ids  (xfer_ids[blk]);
+      std::vector<Number>       &vals (recv_vals[blk]);
+
+      // we now know the number of values we will store for each block,
+      // so we can do efficient preallocation
+      ids.clear(); /**/ ids.reserve (xfer_ids_size[blk]);
+      vals.resize(recv_vals_size[blk]);
+
+      if (recv_vals_size[blk] != 0) // only if there are nonzero values to receive
+	for (iterator_type it=begin; it!=end; ++it)
+	  if (((*it)->id() >= first_object) && // object in [first_object,last_object)
+	      ((*it)->id() <   last_object))
+	    {
+	      ids.push_back((*it)->id());
+	      
+	      unsigned int n_comp_tot=0;
+	    
+	      for (std::vector<unsigned int>::const_iterator var_it=_written_var_indices.begin();
+		   var_it!=_written_var_indices.end(); ++var_it)
+		n_comp_tot += (*it)->n_comp(sys_num,*var_it);
+	      
+	      ids.push_back (n_comp_tot);
+	    }
+
+      Parallel::MessageTag id_tag  = Parallel::Communicator_World.get_unique_tag(100*num_blks + blk);
+      Parallel::MessageTag val_tag = Parallel::Communicator_World.get_unique_tag(200*num_blks + blk);
+      
+      // nonblocking send the data for this block
+      Parallel::send (0, ids,  id_requests[blk],  id_tag);
+      
+      // Go ahead and post the receive too
+      Parallel::receive (0, vals, val_requests[blk], val_tag);
+    }
+
+  //---------------------------------------------------
+  // Here processor 0 will read and distribute the data.
+  // We have to do this block-wise to ensure that we 
+  // do not exhaust memory on processor 0.
+  
+  // give these variables scope outside the block for efficient allocation
+  std::vector<std::vector<unsigned int> > recv_ids       (libMesh::n_processors());
+  std::vector<std::vector<Number> >       send_vals      (libMesh::n_processors());
+  std::vector<Parallel::Request>          reply_requests (libMesh::n_processors());
+  std::vector<unsigned int>               obj_val_offsets;          // map to traverse entry-wise rather than processor-wise
+  std::vector<Number>                     input_vals;               // The input buffer for the current block
+  
+  for (unsigned int blk=0; blk<num_blks; blk++)
+    {
+      // Each processor should build up its transfer buffers for its
+      // local objects in [first_object,last_object).
+      const unsigned int 
+	first_object  = blk*io_blksize,
+	last_object   = std::min((blk+1)*io_blksize,n_objects),
+	n_objects_blk = last_object - first_object;
+      
+      // Processor 0 has a special job.  It needs to gather the requested indices
+      // in [first_object,last_object) from all processors, read the data from
+      // disk, and reply 
+      if (libMesh::processor_id() == 0)
+	{
+	  Parallel::MessageTag id_tag  = Parallel::Communicator_World.get_unique_tag(100*num_blks + blk);
+	  Parallel::MessageTag val_tag = Parallel::Communicator_World.get_unique_tag(200*num_blks + blk);
+
+	  // offset array. this will define where each object's values
+	  // map into the actual input_vals buffer.  this must get
+	  // 0-initialized because 0-component objects are not actually sent
+	  obj_val_offsets.resize (n_objects_blk); /**/ std::fill (obj_val_offsets.begin(), obj_val_offsets.end(), 0);
+	  recv_vals_size.resize(libMesh::n_processors()); // reuse this to count how many values are going to each processor
+				
+	  // loop over all processors and process their index request
+	  for (unsigned int comm_step=0; comm_step<libMesh::n_processors(); comm_step++)
+	    {
+	      // blocking receive indices for this block, imposing no particular order on processor
+	      Parallel::Status id_status (Parallel::probe (Parallel::any_source, id_tag));
+	      std::vector<unsigned int> &ids (recv_ids[id_status.source()]);
+	      unsigned int &n_vals_proc (recv_vals_size[id_status.source()]);
+	      Parallel::receive (id_status.source(), ids,  id_tag);
+	      
+	      n_vals_proc = 0;
+
+	      // note its possible we didn't receive values for objects in
+	      // this block if they have no components allocated.
+	      for (unsigned int idx=0; idx<ids.size(); idx+=2)
+		{
+		  const unsigned int 
+		    local_idx  = ids[idx+0]-first_object,
+		    n_comp_tot = ids[idx+1];
+		  
+		  libmesh_assert_less (local_idx, n_objects_blk);
+		  
+		  obj_val_offsets[local_idx] = n_comp_tot;
+		  n_vals_proc += n_comp_tot;
+		}
+	    }
+
+	  // We need the offests into the input_vals vector for each object.
+	  // fortunately, this is simply the partial sum of the total number 
+	  // of components for each object
+	  std::partial_sum(obj_val_offsets.begin(), obj_val_offsets.end(),
+			   obj_val_offsets.begin());
+
+	  // *** simulated read ***
+	  input_vals.resize(obj_val_offsets.back());
+
+	  // pack data replies for each processor
+ 	  for (unsigned int proc=0; proc<libMesh::n_processors(); proc++)
+	    {
+	      const std::vector<unsigned int> &ids (recv_ids[proc]);
+	      std::vector<Number> &vals (send_vals[proc]);
+	      const unsigned int &n_vals_proc (recv_vals_size[proc]);
+	      
+	      vals.clear(); /**/ vals.reserve(n_vals_proc);
+
+	      for (unsigned int idx=0; idx<ids.size(); idx+=2)
+		{
+		  const unsigned int 
+		    local_idx  = ids[idx+0]-first_object,
+		    n_comp_tot = ids[idx+1];
+		  std::vector<Number>::const_iterator in_vals(input_vals.begin());
+		  std::advance (in_vals, obj_val_offsets[local_idx]);
+
+		  for (unsigned int comp=0; comp<n_comp_tot; comp++, ++in_vals)
+		    vals.push_back(*in_vals);
+		}
+	      
+	      // send the relevant values to this processor
+	      Parallel::send (proc, vals, reply_requests[proc], val_tag);
+	    }	  
+	} // end processor 0 read/reply
+
+      // all processors complete the (already posted) read for this block
+      {
+	Parallel::wait (val_requests[blk]);
+
+	std::vector<Number>::const_iterator val_it(recv_vals[blk].begin());
+	
+	if (!recv_vals[blk].empty()) // nonzero values to receive
+	  for (iterator_type it=begin; it!=end; ++it)
+	    if (((*it)->id() >= first_object) && // object in [first_object,last_object)
+		((*it)->id() <   last_object))
+	      for (std::vector<unsigned int>::const_iterator var_it=_written_var_indices.begin();
+		   var_it!=_written_var_indices.end(); ++var_it)
+		{
+		  const unsigned int n_comp = (*it)->n_comp(sys_num,*var_it);
+		  
+		  for (unsigned int comp=0; comp<n_comp; comp++)
+		    // set the values recieved from processor 0
+		    ;
+		}
+      }
+
+      // processor 0 needs to make sure all replies have been handed off
+      if (libMesh::processor_id () == 0)
+	Parallel::wait(reply_requests);	  
+    }
+
+
+  return n_read_values;
+}
+
+
+
 unsigned int System::read_SCALAR_dofs (const unsigned int var,
                                        Xdr &io,
                                        NumericVector<Number> &vec) const
@@ -894,7 +1152,7 @@
 
 
 
-void System::read_serialized_vector (Xdr& io, NumericVector<Number>& vec)
+unsigned int System::read_serialized_vector (Xdr& io, NumericVector<Number>& vec)
 {
   parallel_only();
 
@@ -966,6 +1224,28 @@
   Parallel::sum (n_assigned_vals);
   libmesh_assert_equal_to (n_assigned_vals, vector_length);
 #endif
+
+  // new implementation
+  {
+    //---------------------------------
+    // Collect the values for all nodes
+    this->read_serialized_blocked_dof_objects (this->get_mesh().n_nodes(),
+					       this->get_mesh().local_nodes_begin(),
+					       this->get_mesh().local_nodes_end(),
+					       io,
+					       vec);
+
+    
+    //------------------------------------
+    // Collect the values for all elements
+    this->read_serialized_blocked_dof_objects (this->get_mesh().n_elem(),
+					       this->get_mesh().local_elements_begin(),
+					       this->get_mesh().local_elements_end(),
+					       io,
+					       vec);
+  }
+
+  return vector_length;
 }
 
 
@@ -1178,214 +1458,6 @@
 
 
 
-// void System::write_data (Xdr& io,
-// 			 const bool write_additional_data) const
-// {
-//   // This is deprecated -- use write_serialized_data() instead.  This will be kept for reference
-//   // for a little while, then dropped.  There is no need call this method any more.
-//   libmesh_deprecated();
-
-//   /**
-//    * This method implements the output of the vectors
-//    * contained in this System object, embedded in the
-//    * output of an EquationSystems<T_sys>.
-//    *
-//    *   9.) The global solution vector, re-ordered to be node-major
-//    *       (More on this later.)
-//    *
-//    *      for each additional vector in the object
-//    *
-//    *      10.) The global additional vector, re-ordered to be
-//    *           node-major (More on this later.)
-//    *
-//    * Note that the actual IO is handled through the Xdr class
-//    * (to be renamed later?) which provides a uniform interface to
-//    * both the XDR (eXternal Data Representation) interface and standard
-//    * ASCII output.  Thus this one section of code will read XDR or ASCII
-//    * files with no changes.
-//    */
-//   libmesh_assert (io.writing());
-
-//   const unsigned int proc_id = this->get_mesh().processor_id();
-
-//   std::string comment;
-
-//   // All processors contribute numeric vector values
-//   std::vector<Number> global_vector;
-
-//   // Collect the global solution on one processor
-//   this->solution->localize_to_one (global_vector, 0);
-
-//   // Only processor 0 actually writes out the soltuion vector.
-//   if (proc_id == 0)
-//     {
-//       // First we need to re-order the solution so that it
-//       // is dof_map agnostic.  This is necessary so that the
-//       // vector might be re-read with a different partitioning
-//       // or DOF distribution.
-//       //
-//       // Currently the vector is written in node-major order.
-//       std::vector<Number> reordered_soln(global_vector.size());
-
-//       unsigned int cnt=0;
-
-//       const unsigned int sys_num = this->number();
-//       const unsigned int n_vars  = this->n_vars();
-
-//       // Build a set of non subactive node indices.
-//       std::set<unsigned int> not_subactive_node_ids;
-//       MeshTools::get_not_subactive_node_ids(this->get_mesh(), not_subactive_node_ids);
-
-//       // Loop over each variable and each node, and write out the value.
-//       for (unsigned int var=0; var<n_vars; var++)
-//         {
-// 	  // First write the nodal DOF values
-//           std::set<unsigned int>::iterator it = not_subactive_node_ids.begin();
-//           const std::set<unsigned int>::iterator end = not_subactive_node_ids.end();
-
-//           for (; it != end; ++it)
-//           {
-//             // Get the global index of this node
-//             const unsigned int node = *it;
-// 	    for (unsigned int index=0; index<this->get_mesh().node(node).n_comp(sys_num, var); index++)
-// 	      {
-//                 libmesh_assert_equal_to (this->get_mesh().node(node).id(), node);
-// 		libmesh_assert_not_equal_to (this->get_mesh().node(node).dof_number(sys_num, var, index),
-// 			                     DofObject::invalid_id);
-// 		libmesh_assert_less (cnt, reordered_soln.size());
-
-// 		reordered_soln[cnt++] =
-// 		  global_vector[this->get_mesh().node(node).dof_number(sys_num, var, index)];
-// 	      }
-//           }
-
-// 	  // Then write the element DOF values
-// 	  {
-// 	    MeshBase::const_element_iterator
-// 	      it  = this->get_mesh().active_elements_begin(),
-// 	      end = this->get_mesh().active_elements_end();
-
-// 	    for (; it!=end; ++it)
-// 	      for (unsigned int index=0; index<(*it)->n_comp(sys_num, var); index++)
-// 	        {
-// 		  libmesh_assert_not_equal_to ((*it)->dof_number(sys_num, var, index),
-// 			                       DofObject::invalid_id);
-
-// 		  libmesh_assert_less (cnt, reordered_soln.size());
-
-// 		  reordered_soln[cnt++] =
-// 		      global_vector[(*it)->dof_number(sys_num, var, index)];
-// 		}
-// 	  }
-// 	}
-
-//       // 9.)
-//       //
-//       // Actually write the reordered solution vector
-//       // for the ith system to disk
-
-//       // set up the comment
-//       {
-// 	comment = "# System \"";
-// 	comment += this->name();
-// 	comment += "\" Solution Vector";
-//       }
-
-//       io.data (reordered_soln, comment.c_str());
-//     }
-
-//   // Only write additional vectors if wanted
-//   if (write_additional_data)
-//     {
-//       std::map<std::string, NumericVector<Number>* >::const_iterator
-// 	pos = _vectors.begin();
-
-//       for(; pos != this->_vectors.end(); ++pos)
-//         {
-// 	  // fill with zero.  In general, a resize is not necessary
-// 	  std::fill (global_vector.begin(), global_vector.end(), libMesh::zero);
-
-// 	  // Collect the global solution on one processor
-// 	  pos->second->localize_to_one (global_vector, 0);
-
-// 	  // Only processor 0 actually writes out the  vector.
-// 	  if (proc_id == 0)
-// 	    {
-// 	      // First we need to re-order the solution so that it
-// 	      // is dof_map agnostic.  This is necessary so that the
-// 	      // vector might be re-read with a different partitioning
-// 	      // or DOF distribution.
-// 	      //
-// 	      // Currently the vector is written in node-major order.
-// 	      std::vector<Number> reordered_soln(global_vector.size());
-
-// 	      unsigned int cnt=0;
-
-// 	      const unsigned int sys_num = this->number();
-// 	      const unsigned int n_vars  = this->n_vars();
-
-// 	      for (unsigned int var=0; var<n_vars; var++)
-// 		{
-// 		  // First write the nodal DOF values
-// 		  {
-// 		    MeshBase::const_node_iterator
-// 		      it  = this->get_mesh().nodes_begin(),
-// 		      end = this->get_mesh().nodes_end();
-
-// 		    for (; it !=end; ++it)
-// 		      for (unsigned int index=0; index<(*it)->n_comp(sys_num, var); index++)
-// 			{
-// 			  libmesh_assert_not_equal_to ((*it)->dof_number(sys_num, var, index),
-// 				                       DofObject::invalid_id);
-// 			  libmesh_assert_less (cnt, reordered_soln.size());
-
-// 			  reordered_soln[cnt++] =
-// 			    global_vector[(*it)->dof_number(sys_num, var, index)];
-// 			}
-// 		  }
-
-// 		  // Then write the element DOF values
-// 		  {
-// 		    MeshBase::const_element_iterator
-// 		      it  = this->get_mesh().active_elements_begin(),
-// 		      end = this->get_mesh().active_elements_end();
-
-// 		    for (; it!=end; ++it)
-// 		      for (unsigned int index=0; index<(*it)->n_comp(sys_num, var); index++)
-// 			{
-// 			  libmesh_assert_not_equal_to ((*it)->dof_number(sys_num, var, index),
-// 				                       DofObject::invalid_id);
-// 			  libmesh_assert_less (cnt, reordered_soln.size());
-
-// 			  reordered_soln[cnt++] =
-// 			    global_vector[(*it)->dof_number(sys_num, var, index)];
-// 			}
-// 		  }
-// 		}
-
-
-// 	      // 10.)
-// 	      //
-// 	      // Actually write the reordered additional vector
-// 	      // for this system to disk
-
-// 	      // set up the comment
-// 	      {
-// 		comment = "# System \"";
-// 		comment += this->name();
-// 		comment += "\" Additional Vector \"";
-// 		comment += pos->first;
-// 		comment += "\"";
-// 	      }
-
-// 	      io.data (reordered_soln, comment.c_str());
-// 	    }
-// 	}
-//     }
-// }
-
-
-
 void System::write_parallel_data (Xdr &io,
 				  const bool write_additional_data) const
 {
@@ -1408,6 +1480,10 @@
    * ASCII output.  Thus this one section of code will read XDR or ASCII
    * files with no changes.
    */
+  PerfLog pl("IO Performance",false);
+  pl.push("write_parallel_data");
+  unsigned int total_written_size = 0;
+  
   std::string comment;
 
   libmesh_assert (io.writing());
@@ -1504,6 +1580,8 @@
 
   io.data (io_buffer, comment.c_str());
 
+  total_written_size += io_buffer.size();
+
   // Only write additional vectors if wanted
   if (write_additional_data)
     {
@@ -1573,8 +1651,20 @@
 	  }
 
 	  io.data (io_buffer, comment.c_str());
+
+	  total_written_size += io_buffer.size();
 	}
     }
+
+  const Real 
+    dt   = pl.get_elapsed_time(),
+    rate = total_written_size*sizeof(Number)/dt; 
+
+  std::cerr << "Write " << total_written_size << " \"Number\" values\n"
+	    << " Elapsed time = " << dt << '\n'
+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
+  
+  pl.pop("write_parallel_data");
 }
 
 
@@ -1598,7 +1688,12 @@
   parallel_only();
   std::string comment;
 
-  this->write_serialized_vector(io, *this->solution);
+  PerfLog pl("IO Performance",false);
+  pl.push("write_serialized_data");
+  unsigned int total_written_size = 0;
+  
+  total_written_size +=
+    this->write_serialized_vector(io, *this->solution);
 
   // set up the comment
   if (libMesh::processor_id() == 0)
@@ -1618,7 +1713,8 @@
 
       for(; pos != this->_vectors.end(); ++pos)
         {
-	  this->write_serialized_vector(io, *pos->second);
+	  total_written_size +=
+	    this->write_serialized_vector(io, *pos->second);
 
 	  // set up the comment
 	  if (libMesh::processor_id() == 0)
@@ -1632,6 +1728,53 @@
 	    }
 	}
     }
+
+  const Real 
+    dt   = pl.get_elapsed_time(),
+    rate = total_written_size*sizeof(Number)/dt; 
+
+  std::cout << "Write " << total_written_size << " \"Number\" values\n"
+	    << " Elapsed time = " << dt << '\n'
+	    << " Rate = " << rate/1.e6 << "(MB/sec)\n\n";
+  
+  pl.pop("write_serialized_data");
+
+  
+
+  
+  // test the new method
+  {
+    std::vector<std::string> names;
+    std::vector<NumericVector<Number>*> vectors_to_write;
+
+    names.push_back("Solution Vector");
+    vectors_to_write.push_back(this->solution.get());
+    
+    // Only write additional vectors if wanted
+    if (write_additional_data)
+      {
+	std::map<std::string, NumericVector<Number>* >::const_iterator
+	  pos = _vectors.begin();
+	
+	for(; pos != this->_vectors.end(); ++pos)
+	  {
+	    names.push_back("Additional Vector " + pos->first);
+	    vectors_to_write.push_back(pos->second);
+	  }
+      }
+
+    total_written_size =
+      this->write_serialized_vectors (io, names, vectors_to_write);
+
+    const Real 
+      dt2   = pl.get_elapsed_time(),
+      rate2 = total_written_size*sizeof(Number)/(dt2-dt);
+    
+    std::cout << "Write (new) " << total_written_size << " \"Number\" values\n"
+	      << " Elapsed time = " << (dt2-dt) << '\n'
+	      << " Rate = " << rate2/1.e6 << "(MB/sec)\n\n";
+
+  }
 }
 
 
@@ -1804,6 +1947,215 @@
   return written_length;
 }
 
+
+
+template <typename iterator_type>
+unsigned int System::write_serialized_blocked_dof_objects (const NumericVector<Number> &vec,
+							   const unsigned int n_objects,
+							   const iterator_type begin,
+							   const iterator_type end,
+							   Xdr &io,
+							   const unsigned int var_to_write) const
+{
+  const unsigned int
+    sys_num    = this->number(),
+    num_vars   = this->n_vars(),    
+    io_blksize = std::min(max_io_blksize, n_objects),
+    num_blks   = std::ceil(static_cast<double>(n_objects)/static_cast<double>(io_blksize));
+
+  // std::cout << "io_blksize = "    << io_blksize
+  // 	    << ", num_objects = " << n_objects
+  // 	    << ", num_blks = "    << num_blks
+  // 	    << std::endl;
+  
+  unsigned int written_length=0;                                   // The numer of values written.  This will be returned
+  std::vector<std::vector<unsigned int> > xfer_ids(num_blks);      // The global IDs and # of components for the local objects in all blocks
+  std::vector<std::vector<Number> >       send_vals(num_blks);     // The raw values for the local objects in all blocks
+  std::vector<Parallel::Request>
+    id_requests(num_blks), val_requests(num_blks);                 // send request handle for each block
+  
+  // ------------------------------------------------------
+  // First pass - count the number of objects in each block
+  // traverse all the objects and figure out which block they
+  // will utlimately live in.
+  std::vector<unsigned int>
+    xfer_ids_size  (num_blks,0),
+    send_vals_size (num_blks,0);
+  
+  for (iterator_type it=begin; it!=end; ++it)
+    {
+      const unsigned int
+	id    = (*it)->id(),
+	block = id/io_blksize;
+      
+      libmesh_assert_less (block, num_blks);
+      
+      xfer_ids_size[block] += 2; // for each object, we store its id, as well as the total number of components for all variables
+
+      for (unsigned int var=0; var<num_vars; var++)
+	send_vals_size[block] += (*it)->n_comp(sys_num, var); // for each variable, we will store the nonzero components
+    }
+
+  //-----------------------------------------
+  // Collect the values for all local objects,
+  // binning them into 'blocks' that will be
+  // sent to processor 0
+  for (unsigned int blk=0; blk<num_blks; blk++)
+    {
+      //libMesh::out << "Writing object block " << blk << std::endl;
+
+      // Each processor should build up its transfer buffers for its
+      // local objects in [first_object,last_object).
+      const unsigned int 
+	first_object = blk*io_blksize,
+	last_object  = std::min((blk+1)*io_blksize,n_objects);
+
+      // convenience
+      std::vector<unsigned int> &ids  (xfer_ids[blk]);
+      std::vector<Number>       &vals (send_vals[blk]);
+
+      // we now know the number of values we will store for each block,
+      // so we can do efficient preallocation
+      ids.clear();  /**/ ids.reserve  (xfer_ids_size[blk]);
+      vals.clear(); /**/ vals.reserve (send_vals_size[blk]);
+
+      if (send_vals_size[blk] != 0) // only send if we have nonzero components to write 
+	for (iterator_type it=begin; it!=end; ++it)
+	  if (((*it)->id() >= first_object) && // object in [first_object,last_object)
+	      ((*it)->id() <   last_object))
+	    {
+	      ids.push_back((*it)->id());
+	      
+	      unsigned int n_comp_tot=0;
+	      
+	      for (unsigned int var=0; var<num_vars; var++)
+		{
+		  const unsigned int n_comp = (*it)->n_comp(sys_num,var);
+		  n_comp_tot += n_comp;
+		  
+		  for (unsigned int comp=0; comp<n_comp; comp++)
+		    {
+		      libmesh_assert_greater_equal ((*it)->dof_number(sys_num, var, comp), vec.first_local_index());
+		      libmesh_assert_less ((*it)->dof_number(sys_num, var, comp), vec.last_local_index());
+		      vals.push_back(vec((*it)->dof_number(sys_num, var, comp)));
+		    }
+		}
+	      ids.push_back (n_comp_tot); // even if 0 - processor 0 has no way of knowing otherwise...		  
+	    }
+      
+      Parallel::MessageTag id_tag  = Parallel::Communicator_World.get_unique_tag(100*num_blks + blk);
+      Parallel::MessageTag val_tag = Parallel::Communicator_World.get_unique_tag(200*num_blks + blk);
+      
+      // nonblocking send the data for this block
+      Parallel::send (0, ids,  id_requests[blk],  id_tag);
+      Parallel::send (0, vals, val_requests[blk], val_tag);
+    }
+
+
+  if (libMesh::processor_id() == 0)
+    {
+      std::vector<std::vector<unsigned int> > recv_ids  (libMesh::n_processors());
+      std::vector<std::vector<Number> >       recv_vals (libMesh::n_processors());
+      std::vector<unsigned int> obj_val_offsets;          // map to traverse entry-wise rather than processor-wise
+      std::vector<Number>       output_vals;              // The output buffer for the current block
+
+      for (unsigned int blk=0; blk<num_blks; blk++)
+	{
+	  // Each processor should build up its transfer buffers for its
+	  // local objects in [first_object,last_object).
+	  const unsigned int 
+	    first_object  = blk*io_blksize,
+	    last_object   = std::min((blk+1)*io_blksize,n_objects),
+	    n_objects_blk = last_object - first_object;
+
+	  // offset array. this will define where each object's values
+	  // map into the actual output_vals buffer.  this must get
+	  // 0-initialized because 0-component objects are not actually sent
+	  obj_val_offsets.resize (n_objects_blk); /**/ std::fill (obj_val_offsets.begin(), obj_val_offsets.end(), 0);
+	  
+	  unsigned int n_val_recvd_blk=0;
+	  
+	  // tags to select data received
+	  Parallel::MessageTag id_tag  (Parallel::Communicator_World.get_unique_tag(100*num_blks + blk));
+	  Parallel::MessageTag val_tag (Parallel::Communicator_World.get_unique_tag(200*num_blks + blk));	      
+	  
+	  // receive this block of data from all processors.
+ 	  for (unsigned int comm_step=0; comm_step<libMesh::n_processors(); comm_step++)
+	    {	      
+	      // blocking receive indices for this block, imposing no particular order on processor
+	      Parallel::Status id_status     (Parallel::probe (Parallel::any_source, id_tag));
+	      std::vector<unsigned int> &ids (recv_ids[id_status.source()]);
+	      Parallel::receive (id_status.source(), ids,  id_tag);
+	      
+	      // note its possible we didn't receive values for objects in
+	      // this block if they have no components allocated.
+	      for (unsigned int idx=0; idx<ids.size(); idx+=2)
+		{
+		  const unsigned int 
+		    local_idx  = ids[idx+0]-first_object,
+		    n_comp_tot = ids[idx+1];
+		  
+		  libmesh_assert_less (local_idx, n_objects_blk);
+		  
+		  obj_val_offsets[local_idx] = n_comp_tot;
+		}
+	      
+	      // blocking receive values for this block, imposing no particular order on processor
+	      Parallel::Status val_status  (Parallel::probe (Parallel::any_source, val_tag));
+	      std::vector<Number> &vals    (recv_vals[val_status.source()]);
+	      Parallel::receive (val_status.source(), vals, val_tag);
+	      
+	      written_length += vals.size();
+
+	      n_val_recvd_blk += vals.size();	      
+	    }
+
+	  // We need the offests into the output_vals vector for each object.
+	  // fortunately, this is simply the partial sum of the total number 
+	  // of components for each object
+	  std::partial_sum(obj_val_offsets.begin(), obj_val_offsets.end(),
+			   obj_val_offsets.begin());
+
+	  // this is the actual output buffer that will be written to disk.
+	  // at ths point we finally know wha size it will be.
+	  output_vals.resize(n_val_recvd_blk);
+	  
+	  // pack data from all processors into output values
+ 	  for (unsigned int proc=0; proc<libMesh::n_processors(); proc++)
+	    {
+	      const std::vector<unsigned int> &ids (recv_ids [proc]);
+	      const std::vector<Number>       &vals(recv_vals[proc]);
+	      std::vector<Number>::const_iterator proc_vals(vals.begin());
+	      
+	      for (unsigned int idx=0; idx<ids.size(); idx+=2)
+		{
+		  const unsigned int 
+		    local_idx  = ids[idx+0]-first_object,
+		    n_comp_tot = ids[idx+1];
+		  
+		  // put this object's data into the proper location
+		  // in  the output buffer
+		  std::vector<Number>::iterator out_vals(output_vals.begin());
+		  std::advance(out_vals, obj_val_offsets[local_idx]);
+		  
+		  for (unsigned int comp=0; comp<n_comp_tot; comp++, ++out_vals, ++proc_vals)
+		    *out_vals = *proc_vals;
+		}
+	    }
+	  
+	  // output_vals buffer is now filled for this block.
+	}
+    }
+
+
+  Parallel::wait(id_requests);
+  Parallel::wait(val_requests);
+  
+  return written_length;
+}
+
+
+
 unsigned int System::write_SCALAR_dofs (const NumericVector<Number> &vec,
                                         const unsigned int var,
 					Xdr &io) const
@@ -1854,7 +2206,8 @@
 }
 
 
-void System::write_serialized_vector (Xdr& io, const NumericVector<Number>& vec) const
+
+unsigned int System::write_serialized_vector (Xdr& io, const NumericVector<Number>& vec) const
 {
   parallel_only();
 
@@ -1900,8 +2253,58 @@
 
   if (libMesh::processor_id() == 0)
     libmesh_assert_equal_to (written_length, vec_length);
+
+  return written_length;
 }
 
+
+
+unsigned int System::write_serialized_vectors (Xdr &io,
+					       const std::vector<std::string> &names,
+					       const std::vector<NumericVector<Number>*> &vectors) const
+{
+  parallel_only();
+
+  libmesh_assert (io.writing());
+
+  // Cache these - they are not free!
+  const unsigned int
+    n_nodes = this->get_mesh().n_nodes(),
+    n_elem  = this->get_mesh().n_elem();  
+
+  unsigned int written_length = 0.;
+  
+  // Loop over each vetor and write it out, object-major
+  for (std::vector<NumericVector<Number>*>::const_iterator vec_it=vectors.begin();
+       vec_it!=vectors.end(); ++vec_it)
+    {
+      libmesh_assert_not_equal_to (*vec_it, NULL);
+      const NumericVector<Number> &vec(**vec_it);
+
+      //---------------------------------
+      // Collect the values for all nodes
+      written_length +=
+	this->write_serialized_blocked_dof_objects (vec,
+						    n_nodes,
+						    this->get_mesh().local_nodes_begin(),
+						    this->get_mesh().local_nodes_end(),
+						    io);
+
+      //------------------------------------
+      // Collect the values for all elements
+      written_length +=
+	this->write_serialized_blocked_dof_objects (vec,
+						    n_elem,
+						    this->get_mesh().local_elements_begin(),
+						    this->get_mesh().local_elements_end(),
+						    io);
+
+      // and finally any scalars
+    }
+      
+  return written_length;
+}
+
 } // namespace libMesh
 
 
